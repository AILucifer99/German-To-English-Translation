{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy import array\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "from unicodedata import normalize\n",
        "\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        " \n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in  lines]\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [word.translate(table) for word in line]\n",
        "            # remove non-printable chars form each token\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "            cleaned.append(clean_pair)\n",
        "    return array(cleaned)\n",
        " \n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        " \n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/drive/MyDrive/NMT-English-to-German/Dataset/deu.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-german.pkl')\n",
        "# spot check\n",
        "for i in range(10):\n",
        "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-german.pkl')\n",
        " \n",
        "# reduce dataset size\n",
        "n_sentences = 25000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:20000], dataset[20000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-german-both.pkl')\n",
        "save_clean_data(train, 'english-german-train.pkl')\n",
        "save_clean_data(test, 'english-german-test.pkl')"
      ],
      "metadata": {
        "id": "HqIXn_FcTWJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f91c061-89be-4af2-d6a7-7c02f0b9404c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german.pkl\n",
            "[hi] => [hallo]\n",
            "[hi] => [hallo]\n",
            "[hi] => [gru gott]\n",
            "[hi] => [gru gott]\n",
            "[run] => [lauf]\n",
            "[run] => [lauf]\n",
            "[wow] => [potzdonner]\n",
            "[wow] => [potzdonner]\n",
            "[wow] => [donnerwetter]\n",
            "[wow] => [donnerwetter]\n",
            "Saved: english-german-both.pkl\n",
            "Saved: english-german-train.pkl\n",
            "Saved: english-german-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        " \n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')"
      ],
      "metadata": {
        "id": "ZBTedbWUSFV4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "5A3kZX-VSSXH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "\n",
        "print('The English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('The English Max Length: %d' % (eng_length))\n",
        "\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('The German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('The German Max Length: %d' % (ger_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3Z_YIRVSksH",
        "outputId": "7191ff72-7a75-4f5d-cd34-fe938892e88c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The English Vocabulary Size: 2814\n",
            "The English Max Length: 5\n",
            "The German Vocabulary Size: 4499\n",
            "The German Max Length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X"
      ],
      "metadata": {
        "id": "UFCMEihhStfg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = tf.keras.utils.to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y"
      ],
      "metadata": {
        "id": "-2WD26fPTv2Y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare training data\n",
        "X_train = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "y_train = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "y_train = encode_output(y_train, eng_vocab_size)\n",
        "\n",
        "# prepare validation data\n",
        "X_test = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "y_test = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "y_test = encode_output(y_test, eng_vocab_size)"
      ],
      "metadata": {
        "id": "5_-PCCYbUCk3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "    model.add(tf.keras.layers.LSTM(n_units))\n",
        "    model.add(tf.keras.layers.RepeatVector(tar_timesteps))\n",
        "    model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tar_vocab, activation='softmax')))\n",
        "    return model\n",
        " \n",
        "# Define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False))\n",
        "\n",
        "# summarize defined model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ztiVmpxUbA3",
        "outputId": "c6deeb61-5b20-42f9-a5cf-bfda8aa280c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 256)           1151744   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVect  (None, 5, 256)           0         \n",
            " or)                                                             \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 5, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 5, 2814)          723198    \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,925,566\n",
            "Trainable params: 2,925,566\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "filename = 'Model.h5'\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filename, monitor='val_loss', \n",
        "                                                verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
        "                                                  patience=10)\n",
        "\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=1024, \n",
        "                    batch_size=1024, \n",
        "                    validation_split=0.25, \n",
        "                    callbacks=[checkpoint, early_stopping],)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBJnRRz1U4qo",
        "outputId": "77c3104b-51c6-4d48-d0d1-5a153a26b703"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 7.5989\n",
            "Epoch 1: val_loss improved from inf to 5.65841, saving model to Model.h5\n",
            "15/15 [==============================] - 10s 312ms/step - loss: 7.5989 - val_loss: 5.6584\n",
            "Epoch 2/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 4.7762\n",
            "Epoch 2: val_loss improved from 5.65841 to 4.56897, saving model to Model.h5\n",
            "15/15 [==============================] - 3s 193ms/step - loss: 4.7762 - val_loss: 4.5690\n",
            "Epoch 3/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 4.4116\n",
            "Epoch 3: val_loss improved from 4.56897 to 4.37043, saving model to Model.h5\n",
            "15/15 [==============================] - 2s 150ms/step - loss: 4.4116 - val_loss: 4.3704\n",
            "Epoch 4/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 4.2257\n",
            "Epoch 4: val_loss improved from 4.37043 to 4.20816, saving model to Model.h5\n",
            "15/15 [==============================] - 2s 124ms/step - loss: 4.2257 - val_loss: 4.2082\n",
            "Epoch 5/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 4.0796\n",
            "Epoch 5: val_loss improved from 4.20816 to 4.07788, saving model to Model.h5\n",
            "15/15 [==============================] - 2s 128ms/step - loss: 4.0796 - val_loss: 4.0779\n",
            "Epoch 6/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.9616\n",
            "Epoch 6: val_loss improved from 4.07788 to 3.97376, saving model to Model.h5\n",
            "15/15 [==============================] - 2s 95ms/step - loss: 3.9587 - val_loss: 3.9738\n",
            "Epoch 7/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 3.8645\n",
            "Epoch 7: val_loss improved from 3.97376 to 3.89158, saving model to Model.h5\n",
            "15/15 [==============================] - 2s 110ms/step - loss: 3.8631 - val_loss: 3.8916\n",
            "Epoch 8/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.7927\n",
            "Epoch 8: val_loss improved from 3.89158 to 3.83414, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 84ms/step - loss: 3.7927 - val_loss: 3.8341\n",
            "Epoch 9/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.7394\n",
            "Epoch 9: val_loss improved from 3.83414 to 3.78965, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 96ms/step - loss: 3.7394 - val_loss: 3.7897\n",
            "Epoch 10/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 3.6946\n",
            "Epoch 10: val_loss improved from 3.78965 to 3.75595, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 3.6952 - val_loss: 3.7560\n",
            "Epoch 11/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.6564\n",
            "Epoch 11: val_loss improved from 3.75595 to 3.72903, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 3.6564 - val_loss: 3.7290\n",
            "Epoch 12/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.6253\n",
            "Epoch 12: val_loss improved from 3.72903 to 3.71094, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 68ms/step - loss: 3.6253 - val_loss: 3.7109\n",
            "Epoch 13/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.5954\n",
            "Epoch 13: val_loss improved from 3.71094 to 3.68854, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 68ms/step - loss: 3.5954 - val_loss: 3.6885\n",
            "Epoch 14/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.5704\n",
            "Epoch 14: val_loss improved from 3.68854 to 3.67330, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 69ms/step - loss: 3.5704 - val_loss: 3.6733\n",
            "Epoch 15/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.5500\n",
            "Epoch 15: val_loss improved from 3.67330 to 3.65783, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 3.5500 - val_loss: 3.6578\n",
            "Epoch 16/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.5298\n",
            "Epoch 16: val_loss improved from 3.65783 to 3.64284, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 3.5298 - val_loss: 3.6428\n",
            "Epoch 17/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.5106\n",
            "Epoch 17: val_loss improved from 3.64284 to 3.62936, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 3.5106 - val_loss: 3.6294\n",
            "Epoch 18/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.4889\n",
            "Epoch 18: val_loss improved from 3.62936 to 3.61431, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 85ms/step - loss: 3.4936 - val_loss: 3.6143\n",
            "Epoch 19/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 3.4771\n",
            "Epoch 19: val_loss improved from 3.61431 to 3.60252, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 3.4748 - val_loss: 3.6025\n",
            "Epoch 20/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.4580\n",
            "Epoch 20: val_loss improved from 3.60252 to 3.58925, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 55ms/step - loss: 3.4580 - val_loss: 3.5893\n",
            "Epoch 21/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.4387\n",
            "Epoch 21: val_loss improved from 3.58925 to 3.57571, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 3.4387 - val_loss: 3.5757\n",
            "Epoch 22/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.4171\n",
            "Epoch 22: val_loss improved from 3.57571 to 3.56245, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 55ms/step - loss: 3.4208 - val_loss: 3.5624\n",
            "Epoch 23/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.4018\n",
            "Epoch 23: val_loss improved from 3.56245 to 3.54548, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 3.4018 - val_loss: 3.5455\n",
            "Epoch 24/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 3.3857\n",
            "Epoch 24: val_loss improved from 3.54548 to 3.53051, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 3.3831 - val_loss: 3.5305\n",
            "Epoch 25/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.3645\n",
            "Epoch 25: val_loss improved from 3.53051 to 3.51679, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 66ms/step - loss: 3.3645 - val_loss: 3.5168\n",
            "Epoch 26/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.3446\n",
            "Epoch 26: val_loss improved from 3.51679 to 3.50130, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 70ms/step - loss: 3.3446 - val_loss: 3.5013\n",
            "Epoch 27/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 3.3275\n",
            "Epoch 27: val_loss improved from 3.50130 to 3.48505, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 3.3254 - val_loss: 3.4850\n",
            "Epoch 28/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 3.3091\n",
            "Epoch 28: val_loss improved from 3.48505 to 3.46550, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 56ms/step - loss: 3.3058 - val_loss: 3.4655\n",
            "Epoch 29/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.2842\n",
            "Epoch 29: val_loss improved from 3.46550 to 3.45090, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 3.2842 - val_loss: 3.4509\n",
            "Epoch 30/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.2621\n",
            "Epoch 30: val_loss improved from 3.45090 to 3.43249, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 3.2621 - val_loss: 3.4325\n",
            "Epoch 31/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.2400\n",
            "Epoch 31: val_loss improved from 3.43249 to 3.41624, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 3.2400 - val_loss: 3.4162\n",
            "Epoch 32/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.2179\n",
            "Epoch 32: val_loss improved from 3.41624 to 3.39738, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 68ms/step - loss: 3.2179 - val_loss: 3.3974\n",
            "Epoch 33/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.1921\n",
            "Epoch 33: val_loss improved from 3.39738 to 3.37968, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 68ms/step - loss: 3.1921 - val_loss: 3.3797\n",
            "Epoch 34/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.1671\n",
            "Epoch 34: val_loss improved from 3.37968 to 3.35714, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 3.1671 - val_loss: 3.3571\n",
            "Epoch 35/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.1403\n",
            "Epoch 35: val_loss improved from 3.35714 to 3.33732, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 3.1403 - val_loss: 3.3373\n",
            "Epoch 36/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.1128\n",
            "Epoch 36: val_loss improved from 3.33732 to 3.31479, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 3.1128 - val_loss: 3.3148\n",
            "Epoch 37/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.0850\n",
            "Epoch 37: val_loss improved from 3.31479 to 3.28998, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 3.0850 - val_loss: 3.2900\n",
            "Epoch 38/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 3.0570\n",
            "Epoch 38: val_loss improved from 3.28998 to 3.26590, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 3.0544 - val_loss: 3.2659\n",
            "Epoch 39/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 3.0219\n",
            "Epoch 39: val_loss improved from 3.26590 to 3.23486, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 66ms/step - loss: 3.0219 - val_loss: 3.2349\n",
            "Epoch 40/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.9836\n",
            "Epoch 40: val_loss improved from 3.23486 to 3.20149, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 55ms/step - loss: 2.9836 - val_loss: 3.2015\n",
            "Epoch 41/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.9441\n",
            "Epoch 41: val_loss improved from 3.20149 to 3.16530, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 2.9431 - val_loss: 3.1653\n",
            "Epoch 42/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.9029\n",
            "Epoch 42: val_loss improved from 3.16530 to 3.13202, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 2.9029 - val_loss: 3.1320\n",
            "Epoch 43/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.8593\n",
            "Epoch 43: val_loss improved from 3.13202 to 3.09593, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 2.8604 - val_loss: 3.0959\n",
            "Epoch 44/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.8200\n",
            "Epoch 44: val_loss improved from 3.09593 to 3.06279, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 2.8200 - val_loss: 3.0628\n",
            "Epoch 45/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.7787\n",
            "Epoch 45: val_loss improved from 3.06279 to 3.03363, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 68ms/step - loss: 2.7787 - val_loss: 3.0336\n",
            "Epoch 46/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.7395\n",
            "Epoch 46: val_loss improved from 3.03363 to 2.99902, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 2.7395 - val_loss: 2.9990\n",
            "Epoch 47/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.6993\n",
            "Epoch 47: val_loss improved from 2.99902 to 2.96616, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 2.6993 - val_loss: 2.9662\n",
            "Epoch 48/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 2.6622\n",
            "Epoch 48: val_loss improved from 2.96616 to 2.93382, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 2.6610 - val_loss: 2.9338\n",
            "Epoch 49/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.6242\n",
            "Epoch 49: val_loss improved from 2.93382 to 2.90685, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 2.6242 - val_loss: 2.9069\n",
            "Epoch 50/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.5810\n",
            "Epoch 50: val_loss improved from 2.90685 to 2.87112, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 66ms/step - loss: 2.5810 - val_loss: 2.8711\n",
            "Epoch 51/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.5413\n",
            "Epoch 51: val_loss improved from 2.87112 to 2.83488, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 55ms/step - loss: 2.5413 - val_loss: 2.8349\n",
            "Epoch 52/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.4994\n",
            "Epoch 52: val_loss improved from 2.83488 to 2.80294, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 2.5003 - val_loss: 2.8029\n",
            "Epoch 53/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.4584\n",
            "Epoch 53: val_loss improved from 2.80294 to 2.76941, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 2.4584 - val_loss: 2.7694\n",
            "Epoch 54/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.4183\n",
            "Epoch 54: val_loss improved from 2.76941 to 2.73849, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 2.4183 - val_loss: 2.7385\n",
            "Epoch 55/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.3769\n",
            "Epoch 55: val_loss improved from 2.73849 to 2.70596, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 2.3769 - val_loss: 2.7060\n",
            "Epoch 56/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 2.3397\n",
            "Epoch 56: val_loss improved from 2.70596 to 2.66913, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 55ms/step - loss: 2.3381 - val_loss: 2.6691\n",
            "Epoch 57/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.2934\n",
            "Epoch 57: val_loss improved from 2.66913 to 2.63335, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 2.2934 - val_loss: 2.6334\n",
            "Epoch 58/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.2510\n",
            "Epoch 58: val_loss improved from 2.63335 to 2.59968, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 2.2516 - val_loss: 2.5997\n",
            "Epoch 59/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.2111\n",
            "Epoch 59: val_loss improved from 2.59968 to 2.57104, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 69ms/step - loss: 2.2111 - val_loss: 2.5710\n",
            "Epoch 60/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 2.1729\n",
            "Epoch 60: val_loss improved from 2.57104 to 2.54038, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 2.1743 - val_loss: 2.5404\n",
            "Epoch 61/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.1395\n",
            "Epoch 61: val_loss improved from 2.54038 to 2.50862, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 2.1351 - val_loss: 2.5086\n",
            "Epoch 62/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.0943\n",
            "Epoch 62: val_loss improved from 2.50862 to 2.47318, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 2.0943 - val_loss: 2.4732\n",
            "Epoch 63/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 2.0547\n",
            "Epoch 63: val_loss improved from 2.47318 to 2.44654, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 2.0547 - val_loss: 2.4465\n",
            "Epoch 64/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 2.0166\n",
            "Epoch 64: val_loss improved from 2.44654 to 2.41918, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 2.0157 - val_loss: 2.4192\n",
            "Epoch 65/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.9804\n",
            "Epoch 65: val_loss improved from 2.41918 to 2.39181, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 1.9789 - val_loss: 2.3918\n",
            "Epoch 66/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.9403\n",
            "Epoch 66: val_loss improved from 2.39181 to 2.37483, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 1.9427 - val_loss: 2.3748\n",
            "Epoch 67/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.9090\n",
            "Epoch 67: val_loss improved from 2.37483 to 2.33555, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.9090 - val_loss: 2.3356\n",
            "Epoch 68/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.8716\n",
            "Epoch 68: val_loss improved from 2.33555 to 2.30825, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 1.8716 - val_loss: 2.3082\n",
            "Epoch 69/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.8372\n",
            "Epoch 69: val_loss improved from 2.30825 to 2.28375, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 1.8371 - val_loss: 2.2837\n",
            "Epoch 70/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.8031\n",
            "Epoch 70: val_loss improved from 2.28375 to 2.25646, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.8031 - val_loss: 2.2565\n",
            "Epoch 71/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.7681\n",
            "Epoch 71: val_loss improved from 2.25646 to 2.23429, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 1.7672 - val_loss: 2.2343\n",
            "Epoch 72/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.7390\n",
            "Epoch 72: val_loss improved from 2.23429 to 2.20830, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.7358 - val_loss: 2.2083\n",
            "Epoch 73/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.7015\n",
            "Epoch 73: val_loss improved from 2.20830 to 2.18351, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 1.7015 - val_loss: 2.1835\n",
            "Epoch 74/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.6632\n",
            "Epoch 74: val_loss improved from 2.18351 to 2.16272, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.6654 - val_loss: 2.1627\n",
            "Epoch 75/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.6328\n",
            "Epoch 75: val_loss improved from 2.16272 to 2.13623, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 1.6328 - val_loss: 2.1362\n",
            "Epoch 76/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.6002\n",
            "Epoch 76: val_loss improved from 2.13623 to 2.11217, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.6002 - val_loss: 2.1122\n",
            "Epoch 77/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.5674\n",
            "Epoch 77: val_loss improved from 2.11217 to 2.08799, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.5674 - val_loss: 2.0880\n",
            "Epoch 78/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.5363\n",
            "Epoch 78: val_loss improved from 2.08799 to 2.06518, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 1.5380 - val_loss: 2.0652\n",
            "Epoch 79/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.5070\n",
            "Epoch 79: val_loss improved from 2.06518 to 2.04467, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.5071 - val_loss: 2.0447\n",
            "Epoch 80/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.4760\n",
            "Epoch 80: val_loss improved from 2.04467 to 2.02287, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 1.4765 - val_loss: 2.0229\n",
            "Epoch 81/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.4447\n",
            "Epoch 81: val_loss improved from 2.02287 to 2.00245, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.4447 - val_loss: 2.0024\n",
            "Epoch 82/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.4142\n",
            "Epoch 82: val_loss improved from 2.00245 to 1.98129, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.4142 - val_loss: 1.9813\n",
            "Epoch 83/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.3833\n",
            "Epoch 83: val_loss improved from 1.98129 to 1.95913, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 1.3833 - val_loss: 1.9591\n",
            "Epoch 84/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.3541\n",
            "Epoch 84: val_loss improved from 1.95913 to 1.93912, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.3541 - val_loss: 1.9391\n",
            "Epoch 85/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 1.3250\n",
            "Epoch 85: val_loss improved from 1.93912 to 1.91625, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 1.3251 - val_loss: 1.9163\n",
            "Epoch 86/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.2979\n",
            "Epoch 86: val_loss improved from 1.91625 to 1.90093, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 1.2979 - val_loss: 1.9009\n",
            "Epoch 87/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.2703\n",
            "Epoch 87: val_loss improved from 1.90093 to 1.88041, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 1.2703 - val_loss: 1.8804\n",
            "Epoch 88/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.2416\n",
            "Epoch 88: val_loss improved from 1.88041 to 1.85824, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.2416 - val_loss: 1.8582\n",
            "Epoch 89/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.2151\n",
            "Epoch 89: val_loss improved from 1.85824 to 1.83979, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.2151 - val_loss: 1.8398\n",
            "Epoch 90/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.1900\n",
            "Epoch 90: val_loss improved from 1.83979 to 1.82756, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.1900 - val_loss: 1.8276\n",
            "Epoch 91/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.1675\n",
            "Epoch 91: val_loss improved from 1.82756 to 1.80565, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 1.1675 - val_loss: 1.8057\n",
            "Epoch 92/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.1359\n",
            "Epoch 92: val_loss improved from 1.80565 to 1.78705, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 1.1359 - val_loss: 1.7870\n",
            "Epoch 93/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.1112\n",
            "Epoch 93: val_loss improved from 1.78705 to 1.78498, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 1.1112 - val_loss: 1.7850\n",
            "Epoch 94/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.0883\n",
            "Epoch 94: val_loss improved from 1.78498 to 1.74956, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 1.0883 - val_loss: 1.7496\n",
            "Epoch 95/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.0613\n",
            "Epoch 95: val_loss improved from 1.74956 to 1.73333, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 1.0613 - val_loss: 1.7333\n",
            "Epoch 96/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.0372\n",
            "Epoch 96: val_loss improved from 1.73333 to 1.71677, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 67ms/step - loss: 1.0372 - val_loss: 1.7168\n",
            "Epoch 97/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.0116\n",
            "Epoch 97: val_loss improved from 1.71677 to 1.70506, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 1.0116 - val_loss: 1.7051\n",
            "Epoch 98/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.9890\n",
            "Epoch 98: val_loss improved from 1.70506 to 1.68523, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 68ms/step - loss: 0.9890 - val_loss: 1.6852\n",
            "Epoch 99/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.9648\n",
            "Epoch 99: val_loss improved from 1.68523 to 1.66656, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.9658 - val_loss: 1.6666\n",
            "Epoch 100/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.9445\n",
            "Epoch 100: val_loss improved from 1.66656 to 1.65246, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.9445 - val_loss: 1.6525\n",
            "Epoch 101/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.9250\n",
            "Epoch 101: val_loss improved from 1.65246 to 1.63598, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.9228 - val_loss: 1.6360\n",
            "Epoch 102/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.9017\n",
            "Epoch 102: val_loss improved from 1.63598 to 1.62522, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.9017 - val_loss: 1.6252\n",
            "Epoch 103/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.8798\n",
            "Epoch 103: val_loss improved from 1.62522 to 1.60809, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.8798 - val_loss: 1.6081\n",
            "Epoch 104/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.8600\n",
            "Epoch 104: val_loss improved from 1.60809 to 1.59232, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.8588 - val_loss: 1.5923\n",
            "Epoch 105/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.8388\n",
            "Epoch 105: val_loss improved from 1.59232 to 1.57568, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 0.8388 - val_loss: 1.5757\n",
            "Epoch 106/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.8173\n",
            "Epoch 106: val_loss improved from 1.57568 to 1.56214, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.8173 - val_loss: 1.5621\n",
            "Epoch 107/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.8002\n",
            "Epoch 107: val_loss improved from 1.56214 to 1.54874, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.8002 - val_loss: 1.5487\n",
            "Epoch 108/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.7812\n",
            "Epoch 108: val_loss improved from 1.54874 to 1.53643, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.7812 - val_loss: 1.5364\n",
            "Epoch 109/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.7629\n",
            "Epoch 109: val_loss improved from 1.53643 to 1.52579, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.7651 - val_loss: 1.5258\n",
            "Epoch 110/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.7470\n",
            "Epoch 110: val_loss improved from 1.52579 to 1.51160, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.7470 - val_loss: 1.5116\n",
            "Epoch 111/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.7275\n",
            "Epoch 111: val_loss improved from 1.51160 to 1.49584, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.7275 - val_loss: 1.4958\n",
            "Epoch 112/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.7126\n",
            "Epoch 112: val_loss improved from 1.49584 to 1.48461, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.7095 - val_loss: 1.4846\n",
            "Epoch 113/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.6958\n",
            "Epoch 113: val_loss improved from 1.48461 to 1.47479, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.6958 - val_loss: 1.4748\n",
            "Epoch 114/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.6807\n",
            "Epoch 114: val_loss improved from 1.47479 to 1.46461, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 0.6803 - val_loss: 1.4646\n",
            "Epoch 115/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.6636\n",
            "Epoch 115: val_loss improved from 1.46461 to 1.45379, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.6636 - val_loss: 1.4538\n",
            "Epoch 116/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.6472\n",
            "Epoch 116: val_loss improved from 1.45379 to 1.44334, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.6472 - val_loss: 1.4433\n",
            "Epoch 117/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.6313\n",
            "Epoch 117: val_loss improved from 1.44334 to 1.43242, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.6313 - val_loss: 1.4324\n",
            "Epoch 118/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.6181\n",
            "Epoch 118: val_loss improved from 1.43242 to 1.42407, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.6171 - val_loss: 1.4241\n",
            "Epoch 119/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.6018\n",
            "Epoch 119: val_loss improved from 1.42407 to 1.40854, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.6030 - val_loss: 1.4085\n",
            "Epoch 120/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.5873\n",
            "Epoch 120: val_loss improved from 1.40854 to 1.40066, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 66ms/step - loss: 0.5873 - val_loss: 1.4007\n",
            "Epoch 121/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.5772\n",
            "Epoch 121: val_loss improved from 1.40066 to 1.39628, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.5748 - val_loss: 1.3963\n",
            "Epoch 122/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.5621\n",
            "Epoch 122: val_loss improved from 1.39628 to 1.38105, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.5621 - val_loss: 1.3810\n",
            "Epoch 123/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.5486\n",
            "Epoch 123: val_loss improved from 1.38105 to 1.37695, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 55ms/step - loss: 0.5486 - val_loss: 1.3770\n",
            "Epoch 124/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.5391\n",
            "Epoch 124: val_loss improved from 1.37695 to 1.36290, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.5367 - val_loss: 1.3629\n",
            "Epoch 125/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.5250\n",
            "Epoch 125: val_loss improved from 1.36290 to 1.35755, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.5250 - val_loss: 1.3576\n",
            "Epoch 126/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.5124\n",
            "Epoch 126: val_loss improved from 1.35755 to 1.34757, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.5124 - val_loss: 1.3476\n",
            "Epoch 127/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.4991\n",
            "Epoch 127: val_loss improved from 1.34757 to 1.33997, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.5004 - val_loss: 1.3400\n",
            "Epoch 128/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.4877\n",
            "Epoch 128: val_loss improved from 1.33997 to 1.32874, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.4900 - val_loss: 1.3287\n",
            "Epoch 129/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.4745\n",
            "Epoch 129: val_loss improved from 1.32874 to 1.32409, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.4761 - val_loss: 1.3241\n",
            "Epoch 130/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.4636\n",
            "Epoch 130: val_loss improved from 1.32409 to 1.31583, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.4648 - val_loss: 1.3158\n",
            "Epoch 131/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.4515\n",
            "Epoch 131: val_loss improved from 1.31583 to 1.30874, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.4538 - val_loss: 1.3087\n",
            "Epoch 132/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.4469\n",
            "Epoch 132: val_loss did not improve from 1.30874\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.4469 - val_loss: 1.3111\n",
            "Epoch 133/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.4381\n",
            "Epoch 133: val_loss improved from 1.30874 to 1.29492, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.4386 - val_loss: 1.2949\n",
            "Epoch 134/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.4262\n",
            "Epoch 134: val_loss improved from 1.29492 to 1.28523, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.4256 - val_loss: 1.2852\n",
            "Epoch 135/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.4157\n",
            "Epoch 135: val_loss improved from 1.28523 to 1.28176, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.4165 - val_loss: 1.2818\n",
            "Epoch 136/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.4067\n",
            "Epoch 136: val_loss improved from 1.28176 to 1.27442, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.4067 - val_loss: 1.2744\n",
            "Epoch 137/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.3985\n",
            "Epoch 137: val_loss improved from 1.27442 to 1.26666, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.3985 - val_loss: 1.2667\n",
            "Epoch 138/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3857\n",
            "Epoch 138: val_loss improved from 1.26666 to 1.26581, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.3881 - val_loss: 1.2658\n",
            "Epoch 139/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.3808\n",
            "Epoch 139: val_loss improved from 1.26581 to 1.25826, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.3810 - val_loss: 1.2583\n",
            "Epoch 140/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3707\n",
            "Epoch 140: val_loss improved from 1.25826 to 1.25058, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.3714 - val_loss: 1.2506\n",
            "Epoch 141/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.3627\n",
            "Epoch 141: val_loss improved from 1.25058 to 1.24674, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.3627 - val_loss: 1.2467\n",
            "Epoch 142/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.3547\n",
            "Epoch 142: val_loss improved from 1.24674 to 1.24229, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.3554 - val_loss: 1.2423\n",
            "Epoch 143/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.3494\n",
            "Epoch 143: val_loss improved from 1.24229 to 1.23540, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.3509 - val_loss: 1.2354\n",
            "Epoch 144/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3426\n",
            "Epoch 144: val_loss did not improve from 1.23540\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.3426 - val_loss: 1.2369\n",
            "Epoch 145/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.3349\n",
            "Epoch 145: val_loss improved from 1.23540 to 1.22780, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.3349 - val_loss: 1.2278\n",
            "Epoch 146/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.3287\n",
            "Epoch 146: val_loss improved from 1.22780 to 1.22105, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 0.3287 - val_loss: 1.2210\n",
            "Epoch 147/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.3217\n",
            "Epoch 147: val_loss did not improve from 1.22105\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.3217 - val_loss: 1.2244\n",
            "Epoch 148/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.3174\n",
            "Epoch 148: val_loss improved from 1.22105 to 1.21590, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 0.3178 - val_loss: 1.2159\n",
            "Epoch 149/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.3086\n",
            "Epoch 149: val_loss improved from 1.21590 to 1.21006, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.3086 - val_loss: 1.2101\n",
            "Epoch 150/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.3017\n",
            "Epoch 150: val_loss improved from 1.21006 to 1.20398, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.3032 - val_loss: 1.2040\n",
            "Epoch 151/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2967\n",
            "Epoch 151: val_loss improved from 1.20398 to 1.20053, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.2967 - val_loss: 1.2005\n",
            "Epoch 152/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2886\n",
            "Epoch 152: val_loss improved from 1.20053 to 1.19634, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.2905 - val_loss: 1.1963\n",
            "Epoch 153/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2850\n",
            "Epoch 153: val_loss improved from 1.19634 to 1.19515, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 37ms/step - loss: 0.2863 - val_loss: 1.1952\n",
            "Epoch 154/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2769\n",
            "Epoch 154: val_loss improved from 1.19515 to 1.19334, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.2795 - val_loss: 1.1933\n",
            "Epoch 155/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2744\n",
            "Epoch 155: val_loss improved from 1.19334 to 1.18637, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.2744 - val_loss: 1.1864\n",
            "Epoch 156/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2677\n",
            "Epoch 156: val_loss improved from 1.18637 to 1.18432, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.2680 - val_loss: 1.1843\n",
            "Epoch 157/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.2633\n",
            "Epoch 157: val_loss improved from 1.18432 to 1.18108, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.2636 - val_loss: 1.1811\n",
            "Epoch 158/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2583\n",
            "Epoch 158: val_loss improved from 1.18108 to 1.17612, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 0.2587 - val_loss: 1.1761\n",
            "Epoch 159/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2531\n",
            "Epoch 159: val_loss improved from 1.17612 to 1.17391, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 69ms/step - loss: 0.2531 - val_loss: 1.1739\n",
            "Epoch 160/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.2489\n",
            "Epoch 160: val_loss improved from 1.17391 to 1.16718, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.2495 - val_loss: 1.1672\n",
            "Epoch 161/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2443\n",
            "Epoch 161: val_loss did not improve from 1.16718\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.2443 - val_loss: 1.1679\n",
            "Epoch 162/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2417\n",
            "Epoch 162: val_loss improved from 1.16718 to 1.16375, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.2417 - val_loss: 1.1638\n",
            "Epoch 163/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2334\n",
            "Epoch 163: val_loss improved from 1.16375 to 1.15920, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.2357 - val_loss: 1.1592\n",
            "Epoch 164/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2321\n",
            "Epoch 164: val_loss improved from 1.15920 to 1.15634, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.2321 - val_loss: 1.1563\n",
            "Epoch 165/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2268\n",
            "Epoch 165: val_loss improved from 1.15634 to 1.15357, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 66ms/step - loss: 0.2268 - val_loss: 1.1536\n",
            "Epoch 166/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2232\n",
            "Epoch 166: val_loss improved from 1.15357 to 1.15349, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.2232 - val_loss: 1.1535\n",
            "Epoch 167/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.2204\n",
            "Epoch 167: val_loss improved from 1.15349 to 1.14970, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.2193 - val_loss: 1.1497\n",
            "Epoch 168/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2151\n",
            "Epoch 168: val_loss improved from 1.14970 to 1.14839, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.2172 - val_loss: 1.1484\n",
            "Epoch 169/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.2114\n",
            "Epoch 169: val_loss improved from 1.14839 to 1.14307, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.2124 - val_loss: 1.1431\n",
            "Epoch 170/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.2081\n",
            "Epoch 170: val_loss did not improve from 1.14307\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.2098 - val_loss: 1.1531\n",
            "Epoch 171/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2109\n",
            "Epoch 171: val_loss did not improve from 1.14307\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.2109 - val_loss: 1.1481\n",
            "Epoch 172/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.2067\n",
            "Epoch 172: val_loss improved from 1.14307 to 1.13946, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.2069 - val_loss: 1.1395\n",
            "Epoch 173/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2003\n",
            "Epoch 173: val_loss improved from 1.13946 to 1.13523, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.2003 - val_loss: 1.1352\n",
            "Epoch 174/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1964\n",
            "Epoch 174: val_loss improved from 1.13523 to 1.13418, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.1972 - val_loss: 1.1342\n",
            "Epoch 175/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1929\n",
            "Epoch 175: val_loss improved from 1.13418 to 1.13043, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.1935 - val_loss: 1.1304\n",
            "Epoch 176/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1889\n",
            "Epoch 176: val_loss improved from 1.13043 to 1.12883, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1892 - val_loss: 1.1288\n",
            "Epoch 177/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1869\n",
            "Epoch 177: val_loss improved from 1.12883 to 1.12751, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 0.1869 - val_loss: 1.1275\n",
            "Epoch 178/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1839\n",
            "Epoch 178: val_loss improved from 1.12751 to 1.12609, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.1847 - val_loss: 1.1261\n",
            "Epoch 179/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1814\n",
            "Epoch 179: val_loss improved from 1.12609 to 1.12421, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.1816 - val_loss: 1.1242\n",
            "Epoch 180/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1799\n",
            "Epoch 180: val_loss improved from 1.12421 to 1.12264, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.1788 - val_loss: 1.1226\n",
            "Epoch 181/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1768\n",
            "Epoch 181: val_loss improved from 1.12264 to 1.12094, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.1768 - val_loss: 1.1209\n",
            "Epoch 182/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1743\n",
            "Epoch 182: val_loss improved from 1.12094 to 1.11855, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.1743 - val_loss: 1.1185\n",
            "Epoch 183/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1705\n",
            "Epoch 183: val_loss improved from 1.11855 to 1.11818, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1705 - val_loss: 1.1182\n",
            "Epoch 184/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1684\n",
            "Epoch 184: val_loss did not improve from 1.11818\n",
            "15/15 [==============================] - 1s 48ms/step - loss: 0.1684 - val_loss: 1.1198\n",
            "Epoch 185/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1655\n",
            "Epoch 185: val_loss improved from 1.11818 to 1.11192, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 0.1655 - val_loss: 1.1119\n",
            "Epoch 186/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1636\n",
            "Epoch 186: val_loss did not improve from 1.11192\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.1636 - val_loss: 1.1178\n",
            "Epoch 187/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1617\n",
            "Epoch 187: val_loss did not improve from 1.11192\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.1611 - val_loss: 1.1141\n",
            "Epoch 188/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1587\n",
            "Epoch 188: val_loss improved from 1.11192 to 1.11063, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.1587 - val_loss: 1.1106\n",
            "Epoch 189/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1568\n",
            "Epoch 189: val_loss improved from 1.11063 to 1.10939, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1565 - val_loss: 1.1094\n",
            "Epoch 190/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1557\n",
            "Epoch 190: val_loss improved from 1.10939 to 1.10872, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.1555 - val_loss: 1.1087\n",
            "Epoch 191/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1530\n",
            "Epoch 191: val_loss did not improve from 1.10872\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1530 - val_loss: 1.1091\n",
            "Epoch 192/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1513\n",
            "Epoch 192: val_loss improved from 1.10872 to 1.10402, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.1517 - val_loss: 1.1040\n",
            "Epoch 193/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1495\n",
            "Epoch 193: val_loss did not improve from 1.10402\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1495 - val_loss: 1.1072\n",
            "Epoch 194/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1488\n",
            "Epoch 194: val_loss did not improve from 1.10402\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1488 - val_loss: 1.1044\n",
            "Epoch 195/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1480\n",
            "Epoch 195: val_loss did not improve from 1.10402\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1480 - val_loss: 1.1057\n",
            "Epoch 196/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1465\n",
            "Epoch 196: val_loss improved from 1.10402 to 1.10171, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.1458 - val_loss: 1.1017\n",
            "Epoch 197/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1423\n",
            "Epoch 197: val_loss improved from 1.10171 to 1.10068, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1421 - val_loss: 1.1007\n",
            "Epoch 198/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1404\n",
            "Epoch 198: val_loss improved from 1.10068 to 1.09796, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.1404 - val_loss: 1.0980\n",
            "Epoch 199/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1391\n",
            "Epoch 199: val_loss did not improve from 1.09796\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1391 - val_loss: 1.0989\n",
            "Epoch 200/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1371\n",
            "Epoch 200: val_loss improved from 1.09796 to 1.09618, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.1371 - val_loss: 1.0962\n",
            "Epoch 201/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1358\n",
            "Epoch 201: val_loss did not improve from 1.09618\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1358 - val_loss: 1.1004\n",
            "Epoch 202/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1320\n",
            "Epoch 202: val_loss did not improve from 1.09618\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.1338 - val_loss: 1.0984\n",
            "Epoch 203/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1327\n",
            "Epoch 203: val_loss did not improve from 1.09618\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1327 - val_loss: 1.0986\n",
            "Epoch 204/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1324\n",
            "Epoch 204: val_loss improved from 1.09618 to 1.09566, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1324 - val_loss: 1.0957\n",
            "Epoch 205/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1302\n",
            "Epoch 205: val_loss improved from 1.09566 to 1.09494, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.1302 - val_loss: 1.0949\n",
            "Epoch 206/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1284\n",
            "Epoch 206: val_loss improved from 1.09494 to 1.09462, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 0.1284 - val_loss: 1.0946\n",
            "Epoch 207/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1270\n",
            "Epoch 207: val_loss improved from 1.09462 to 1.09259, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1270 - val_loss: 1.0926\n",
            "Epoch 208/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1246\n",
            "Epoch 208: val_loss did not improve from 1.09259\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1246 - val_loss: 1.0931\n",
            "Epoch 209/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1219\n",
            "Epoch 209: val_loss did not improve from 1.09259\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1234 - val_loss: 1.0928\n",
            "Epoch 210/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1207\n",
            "Epoch 210: val_loss did not improve from 1.09259\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.1222 - val_loss: 1.0934\n",
            "Epoch 211/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1216\n",
            "Epoch 211: val_loss improved from 1.09259 to 1.09015, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 0.1216 - val_loss: 1.0901\n",
            "Epoch 212/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1182\n",
            "Epoch 212: val_loss did not improve from 1.09015\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.1197 - val_loss: 1.0918\n",
            "Epoch 213/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1187\n",
            "Epoch 213: val_loss did not improve from 1.09015\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.1182 - val_loss: 1.0937\n",
            "Epoch 214/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1172\n",
            "Epoch 214: val_loss did not improve from 1.09015\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1172 - val_loss: 1.0902\n",
            "Epoch 215/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1154\n",
            "Epoch 215: val_loss improved from 1.09015 to 1.08992, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1154 - val_loss: 1.0899\n",
            "Epoch 216/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1145\n",
            "Epoch 216: val_loss improved from 1.08992 to 1.08931, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1145 - val_loss: 1.0893\n",
            "Epoch 217/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1139\n",
            "Epoch 217: val_loss did not improve from 1.08931\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1139 - val_loss: 1.0898\n",
            "Epoch 218/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1122\n",
            "Epoch 218: val_loss did not improve from 1.08931\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1120 - val_loss: 1.0900\n",
            "Epoch 219/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1110\n",
            "Epoch 219: val_loss improved from 1.08931 to 1.08624, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.1110 - val_loss: 1.0862\n",
            "Epoch 220/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1086\n",
            "Epoch 220: val_loss did not improve from 1.08624\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.1105 - val_loss: 1.0876\n",
            "Epoch 221/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1109\n",
            "Epoch 221: val_loss did not improve from 1.08624\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1095 - val_loss: 1.0875\n",
            "Epoch 222/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1082\n",
            "Epoch 222: val_loss did not improve from 1.08624\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.1079 - val_loss: 1.0866\n",
            "Epoch 223/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.1059\n",
            "Epoch 223: val_loss did not improve from 1.08624\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1067 - val_loss: 1.0874\n",
            "Epoch 224/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1059\n",
            "Epoch 224: val_loss did not improve from 1.08624\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.1059 - val_loss: 1.0875\n",
            "Epoch 225/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1054\n",
            "Epoch 225: val_loss did not improve from 1.08624\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.1054 - val_loss: 1.0903\n",
            "Epoch 226/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1047\n",
            "Epoch 226: val_loss improved from 1.08624 to 1.08595, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.1047 - val_loss: 1.0860\n",
            "Epoch 227/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1041\n",
            "Epoch 227: val_loss improved from 1.08595 to 1.08573, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.1041 - val_loss: 1.0857\n",
            "Epoch 228/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1019\n",
            "Epoch 228: val_loss did not improve from 1.08573\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.1030 - val_loss: 1.0866\n",
            "Epoch 229/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1006\n",
            "Epoch 229: val_loss did not improve from 1.08573\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.1015 - val_loss: 1.0862\n",
            "Epoch 230/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.1007\n",
            "Epoch 230: val_loss did not improve from 1.08573\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.1007 - val_loss: 1.0867\n",
            "Epoch 231/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0994\n",
            "Epoch 231: val_loss did not improve from 1.08573\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0999 - val_loss: 1.0865\n",
            "Epoch 232/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0988\n",
            "Epoch 232: val_loss improved from 1.08573 to 1.08484, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.0988 - val_loss: 1.0848\n",
            "Epoch 233/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0985\n",
            "Epoch 233: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.0985 - val_loss: 1.0863\n",
            "Epoch 234/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0978\n",
            "Epoch 234: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.0975 - val_loss: 1.0860\n",
            "Epoch 235/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0969\n",
            "Epoch 235: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.0969 - val_loss: 1.0880\n",
            "Epoch 236/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.0964\n",
            "Epoch 236: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.0969 - val_loss: 1.0850\n",
            "Epoch 237/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0963\n",
            "Epoch 237: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0962 - val_loss: 1.0873\n",
            "Epoch 238/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0950\n",
            "Epoch 238: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0957 - val_loss: 1.0855\n",
            "Epoch 239/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0950\n",
            "Epoch 239: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.0950 - val_loss: 1.0849\n",
            "Epoch 240/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.0945\n",
            "Epoch 240: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.0939 - val_loss: 1.0852\n",
            "Epoch 241/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0931\n",
            "Epoch 241: val_loss did not improve from 1.08484\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0931 - val_loss: 1.0889\n",
            "Epoch 242/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0924\n",
            "Epoch 242: val_loss improved from 1.08484 to 1.08393, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.0924 - val_loss: 1.0839\n",
            "Epoch 243/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0911\n",
            "Epoch 243: val_loss did not improve from 1.08393\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.0911 - val_loss: 1.0863\n",
            "Epoch 244/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0909\n",
            "Epoch 244: val_loss improved from 1.08393 to 1.08271, saving model to Model.h5\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 0.0909 - val_loss: 1.0827\n",
            "Epoch 245/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.0894\n",
            "Epoch 245: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.0906 - val_loss: 1.0855\n",
            "Epoch 246/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0897\n",
            "Epoch 246: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.0897 - val_loss: 1.0846\n",
            "Epoch 247/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0886\n",
            "Epoch 247: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.0886 - val_loss: 1.0879\n",
            "Epoch 248/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0882\n",
            "Epoch 248: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0887 - val_loss: 1.0878\n",
            "Epoch 249/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0885\n",
            "Epoch 249: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.0885 - val_loss: 1.0847\n",
            "Epoch 250/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0884\n",
            "Epoch 250: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.0884 - val_loss: 1.0908\n",
            "Epoch 251/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0877\n",
            "Epoch 251: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0885 - val_loss: 1.0881\n",
            "Epoch 252/1024\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 0.0867\n",
            "Epoch 252: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0870 - val_loss: 1.0840\n",
            "Epoch 253/1024\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0858\n",
            "Epoch 253: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 0.0858 - val_loss: 1.0869\n",
            "Epoch 254/1024\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 0.0853\n",
            "Epoch 254: val_loss did not improve from 1.08271\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.0859 - val_loss: 1.0874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "TAF_DfpgPcVz",
        "outputId": "756041ea-29a1-41fd-ee36-2da2e33b6ee8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of the trained LSTM Encoder-Decoder Model"
      ],
      "metadata": {
        "id": "wrCWP5ZyYPVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"
      ],
      "metadata": {
        "id": "AjBGS8_-XM3o"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        " \n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        " \n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        " \n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        " \n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X\n",
        " \n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        " \n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [argmax(vector) for vector in prediction]\n",
        "    target = list()\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)\n",
        " \n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "    actual, predicted = list(), list()\n",
        "    for i, source in enumerate(sources):\n",
        "        # translate encoded source text\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        translation = predict_sequence(model, eng_tokenizer, source)\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        if i < 10:\n",
        "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "        actual.append([raw_target.split()])\n",
        "        predicted.append(translation.split())\n",
        "\n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        " \n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('Model.h5')\n",
        "\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGy0riwKYW1A",
        "outputId": "6d13bb0b-b873-444c-904c-af9e961b827c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "src=[er besitzt ubersinnliche krafte], target=[he is a psychic], predicted=[he is a psychic]\n",
            "src=[versucht es noch mal], target=[try again], predicted=[try it once]\n",
            "src=[druck mich], target=[give me a hug], predicted=[give me a hug]\n",
            "src=[tom hat sich verandert], target=[tom changed], predicted=[tom changed]\n",
            "src=[wie geht es euch heute], target=[how are you today], predicted=[how are you today]\n",
            "src=[hier hast du deinen salat], target=[heres your salad], predicted=[heres your salad]\n",
            "src=[darf ich dir helfen], target=[may i help you], predicted=[can i help you]\n",
            "src=[ich habe eine armbanduhr gekauft], target=[i bought a watch], predicted=[i bought a watch]\n",
            "src=[tut mir leid deswegen], target=[sorry about that], predicted=[sorry about that]\n",
            "src=[lassen sie alles zuruck], target=[leave everything], predicted=[leave everything]\n",
            "BLEU-1: 0.920375\n",
            "BLEU-2: 0.898648\n",
            "BLEU-3: 0.854116\n",
            "BLEU-4: 0.649271\n",
            "test\n",
            "src=[er ist ein groer bub], target=[he is a tall boy], predicted=[hes in good]\n",
            "src=[ich fuhle mich schrecklich], target=[i feel terrible], predicted=[i feel horrible]\n",
            "src=[wir sind studenten], target=[were students], predicted=[we are students]\n",
            "src=[habt ihr tom geliebt], target=[did you love tom], predicted=[did you love tom]\n",
            "src=[wir alle brauchen liebe], target=[we all need love], predicted=[we all need love]\n",
            "src=[tom sang], target=[tom was singing], predicted=[tom was singing]\n",
            "src=[er hat ein lied gesungen], target=[he sang a song], predicted=[he sang a song]\n",
            "src=[er will kommen], target=[he wants to come], predicted=[he wants to come]\n",
            "src=[ihr seid entschlossen], target=[youre decisive], predicted=[youre decisive]\n",
            "src=[ich fuhre selbstgesprache], target=[i talk to myself], predicted=[im am]\n",
            "BLEU-1: 0.782934\n",
            "BLEU-2: 0.728561\n",
            "BLEU-3: 0.683931\n",
            "BLEU-4: 0.491327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZhklOhN0UyIs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}